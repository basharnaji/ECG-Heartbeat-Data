{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading the data and exploring its shape and values\n\nThis notebook is Part 2 of my analysis of the ECG Hearbeat dataset.  In this version I'll be focusing on building Deep Learning models compared to the original version which I tried to use \"standard\" machine leraning models to establish a baseline for whether its worth it to use Deep Learning or not.\n\nThe baseline version can be found [here](https://www.kaggle.com/basharalkuwaiti/ecg-heartbeat-categorization-baseline)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import resample\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-04T04:04:23.570997Z","iopub.execute_input":"2021-07-04T04:04:23.571716Z","iopub.status.idle":"2021-07-04T04:04:24.628469Z","shell.execute_reply.started":"2021-07-04T04:04:23.571589Z","shell.execute_reply":"2021-07-04T04:04:24.627720Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/heartbeat/ptbdb_abnormal.csv\n/kaggle/input/heartbeat/ptbdb_normal.csv\n/kaggle/input/heartbeat/mitbih_test.csv\n/kaggle/input/heartbeat/mitbih_train.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"mit_test = pd.read_csv('/kaggle/input/heartbeat/mitbih_test.csv',header=None)\nmit_train = pd.read_csv('/kaggle/input/heartbeat/mitbih_train.csv', header=None)\nptb_abnormal = pd.read_csv('/kaggle/input/heartbeat/ptbdb_abnormal.csv', header=None)\nptb_normal = pd.read_csv('/kaggle/input/heartbeat/ptbdb_normal.csv', header=None)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T04:04:24.632000Z","iopub.execute_input":"2021-07-04T04:04:24.632949Z","iopub.status.idle":"2021-07-04T04:04:36.605427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mit_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mit_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ptb_abnormal.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ptb_normal.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mit_test.rename(columns={187:\"Class\"}, inplace=True)\nmit_train.rename(columns={187:\"Class\"}, inplace=True)\nptb_abnormal.rename(columns={187:\"Class\"}, inplace=True)\nptb_normal.rename(columns={187:\"Class\"}, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at how many classes are there in each dataset\nThe MIT dataset has 5 clases:\n* 0 = N  (Normal Beat)\n* 1 = S  (Supraventricular premature beat)\n* 2 = V  (Premature ventricular contraction)\n* 3 = F  (Fusion of ventricular and normal beat)\n* 4 = Q  (Unclassifiable beat)\n\nCompared to the PTB dataset which is 1 for abnormal and 0 for normal\n","metadata":{}},{"cell_type":"code","source":"print (\"MIT Train classes: \\n\", mit_train[\"Class\"].value_counts())\nprint (\"\\nMIT Test classes: \\n\", mit_test[\"Class\"].value_counts())\nprint (\"\\nPTB Abnormal classes: \\n\", ptb_abnormal[\"Class\"].value_counts())\nprint (\"\\nPTB Normal classes: \\n\", ptb_normal[\"Class\"].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting Dictionary to define the type of Heartbeat for both datasets\nMIT_Outcome = {0. : 'Normal Beat',\n               1. : 'Supraventricular premature beat',\n               2. : 'Premature ventricular contraction',\n               3. : 'Fusion of ventricular and normal beat',\n               4. : 'Unclassifiable beat'}\nPTB_Outcome = {0. : 'Normal',\n               1. : 'Abnormal'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating Plots of some of the samples in the dataset","metadata":{}},{"cell_type":"code","source":"#Plotting 10 random samples from the MIT training dataset with their classification\nplt.figure(figsize=(25,10))\nnp_count = np.linspace(0,186,187)\nnp_time = np.tile(np_count,(10,1))\nrnd = np.random.randint(0,mit_train.shape[0],size=(10,))\n\n\nfor i in range(np_time.shape[0]):\n    ax = plt.subplot(2,5,i+1)\n    ax.plot(mit_train.iloc[rnd[i],np_time[i,:]])\n    ax.set_title(MIT_Outcome[mit_train.loc[rnd[i],'Class']])\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting 10 random samples from the PTB training dataset with their classification\nplt.figure(figsize=(25,10))\nrnd = np.random.randint(0,ptb_normal.shape[0],size=(5,))\nrnd1 = np.random.randint(0,ptb_abnormal.shape[0], size=(5,))\n\nfor i in range(np_time.shape[0]):\n    ax = plt.subplot(2,5,i+1)\n    if (i < 5):\n        ax.plot(ptb_normal.iloc[rnd[i],np_time[i,:]])\n        ax.set_title(PTB_Outcome[ptb_normal.loc[rnd[i],'Class']])\n    else:\n        ax.plot(ptb_abnormal.iloc[rnd1[i-5],np_time[i,:]])\n        ax.set_title(PTB_Outcome[ptb_abnormal.loc[rnd1[i-5],'Class']])\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep Learning Analysis\n\nThis is the where the notebooks are different.  Tha analysis above is similar to the [Baseline](https://www.kaggle.com/basharalkuwaiti/ecg-heartbeat-categorization-baseline) version","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import classification_report\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Dropout, InputLayer, LSTM, GRU, BatchNormalization, Bidirectional\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.optimizers import SGD","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Preparing the training, validation and test sets for the PTB Data set\n#ptb_abnormal = resample(ptb_abnormal,replace=True,n_samples=ptb_normal.shape[0],random_state=42)\nptb_full = pd.concat([ptb_normal, ptb_abnormal], axis=0).reset_index()\nptb_full.drop(columns='index', inplace=True)\nptb_full = ptb_full.sample(ptb_full.shape[0], random_state=42)\ntrain_ptb, test_ptb, out_train_ptb, out_test_ptb = train_test_split(ptb_full.iloc[:,:187], ptb_full.iloc[:,-1], test_size=0.15, random_state=42)\ntrain_ptb, valid_ptb, out_train_ptb, out_valid_ptb = train_test_split(train_ptb, out_train_ptb, test_size=0.2, random_state=42 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(25,10))\nrnd = np.random.randint(0,train_ptb.shape[0],size=(10,))\n\nfor i in range(np_time.shape[0]):\n    ax = plt.subplot(2,5,i+1)\n    ax.plot(train_ptb.iloc[rnd[i],np_time[i,:]])\n    ax.set_title(PTB_Outcome[out_train_ptb.iloc[rnd[i]]])\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normal, abnormal = np.bincount(ptb_full.loc[:,'Class'])\nnorm_weight = (1/normal) * ((normal+abnormal)/2)\nabnorm_weight = (1/abnormal) * ((normal+abnormal)/2)\nclass_weight = {0: norm_weight, 1: abnorm_weight}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Traing dataset size: \", train_ptb.shape)\nprint(\"Validation dataset size: \", valid_ptb.shape)\nprint(\"Test dataset size: \", test_ptb.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Normalizing the training & test data \ntrain_ptb = normalize(train_ptb, axis=0, norm='max')\nvalid_ptb = normalize(valid_ptb, axis=0, norm='max')\ntest_ptb = normalize(test_ptb, axis=0, norm='max')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 15\nx_train_ptb = train_ptb.reshape(len(train_ptb),train_ptb.shape[1],1)\nx_valid_ptb = valid_ptb.reshape(len(valid_ptb),valid_ptb.shape[1],1)\nx_test_ptb = test_ptb.reshape(len(test_ptb),test_ptb.shape[1],1)\nout_train_ptb = out_train_ptb.values.reshape(len(out_train_ptb), 1)\nout_valid_ptb = out_valid_ptb.values.reshape(len(out_valid_ptb), 1)\nout_test_ptb = out_test_ptb.values.reshape(len(out_test_ptb), 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_ptb.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(25,10))\nrnd = np.random.randint(0,x_train_ptb.shape[0],size=(10,))\n\nfor i in range(np_time.shape[0]):\n    ax = plt.subplot(2,5,i+1)\n    ax.plot(np_time[i,:], x_train_ptb[rnd[i],:,0])\n    ax.set_title(PTB_Outcome[out_train_ptb[rnd[i],0]])\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Traing dataset size: \", x_train_ptb.shape , \" -- Y size: \", out_train_ptb.shape)\nprint(\"Validation dataset size: \", x_valid_ptb.shape , \" -- Y size: \", out_valid_ptb.shape)\nprint(\"Test dataset size: \", x_test_ptb.shape , \" -- Y size: \", out_test_ptb.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\n\n#Function to build Convolutional 1D Networks\ndef build_conv1d_model (input_shape=(x_train_ptb.shape[1],1)):\n    model = keras.models.Sequential()\n    model.add(InputLayer(input_shape=input_shape))\n    \n    model.add(Conv1D(256,7, padding='same'))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(128,7, padding='same'))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n #   model.add(Conv1D(64,7, padding='same'))\n #   model.add(BatchNormalization())\n #   model.add(tf.keras.layers.ReLU())\n #   model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Flatten())\n #   model.add(Dense(512, activation='relu'))\n #   model.add(Dropout(0.5))\n #   model.add(Dense(256, activation='relu'))\n #   model.add(Dropout(0.5))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(1, activation=\"sigmoid\"))\n    model.compile(optimizer=SGD(lr=0.1e-6), loss=\"binary_crossentropy\", metrics=[tfa.metrics.F1Score(2,\"micro\")])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_cb = ModelCheckpoint(\"conv1d_ptb.h5\", save_best_only=True)\n\nearlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n\nmodel_conv1d_ptb= build_conv1d_model(input_shape=(x_train_ptb.shape[1], x_train_ptb.shape[2]))\nmodel_conv1d_ptb.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_conv1d_ptb = model_conv1d_ptb.fit(x_train_ptb, out_train_ptb, epochs=40, batch_size=32, \n                                          class_weight=class_weight, validation_data=(x_valid_ptb, out_valid_ptb),  \n                                          callbacks=[checkpoint_cb, earlystop_cb])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_conv1d_ptb.evaluate(x_test_ptb,out_test_ptb)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conv1d_pred_ptb = model_conv1d_ptb.predict (x_test_ptb)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(out_test_ptb, conv1d_pred_ptb > 0.5, target_names=[PTB_Outcome[i] for i in PTB_Outcome]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = tf.keras.metrics.binary_accuracy(out_test_ptb, conv1d_pred_ptb).numpy()\nprint(\"Binaary Accuracy:  \", m.sum()/len(m))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use a log scale on y-axis to show the wide range of values.\nplt.figure(figsize=(25,12))\nplt.plot(history_conv1d_ptb.epoch, history_conv1d_ptb.history['loss'],\n           color='r', label='Train loss')\nplt.plot(history_conv1d_ptb.epoch, history_conv1d_ptb.history['val_loss'],\n           color='b', label='Val loss' , linestyle=\"--\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.plot(history_conv1d_ptb.epoch, history_conv1d_ptb.history['f1_score'],\n           color='g', label='Train F1')\nplt.plot(history_conv1d_ptb.epoch, history_conv1d_ptb.history['val_f1_score'],\n           color='c', label='Val F1' , linestyle=\"--\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_LSTM_model (n_hidden=1, n_neurons=512, dropout=0.5, input_shape=(x_train_ptb.shape[1],1)):\n    orig_neurons = n_neurons\n    model = keras.models.Sequential()\n    model.add(InputLayer(input_shape=input_shape))\n    \n    model.add(LSTM(128, return_sequences=True, dropout=dropout, recurrent_dropout = dropout))\n    model.add(LSTM(128, return_sequences=True, dropout=dropout, recurrent_dropout = dropout))\n    model.add(LSTM(128, dropout=dropout, recurrent_dropout=dropout))\n    \n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(dropout))\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(dropout))\n    \n    model.add(Dense(1, activation=\"sigmoid\"))\n    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=tfa.metrics.F1Score(2,\"micro\"))\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-03T22:10:12.123430Z","iopub.execute_input":"2021-07-03T22:10:12.123756Z","iopub.status.idle":"2021-07-03T22:10:12.132495Z","shell.execute_reply.started":"2021-07-03T22:10:12.123728Z","shell.execute_reply":"2021-07-03T22:10:12.131824Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"checkpoint_cb = ModelCheckpoint(\"lstm_ptb.h5\", save_best_only=True)\n\nearlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n\nmodel_lstm_ptb = build_LSTM_model(n_neurons = 128, n_hidden=2, dropout=0.2, input_shape=(x_train_ptb.shape[1], x_train_ptb.shape[2]))\nmodel_lstm_ptb.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-03T22:10:12.133563Z","iopub.execute_input":"2021-07-03T22:10:12.134010Z","iopub.status.idle":"2021-07-03T22:10:12.567676Z","shell.execute_reply.started":"2021-07-03T22:10:12.133980Z","shell.execute_reply":"2021-07-03T22:10:12.566764Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm (LSTM)                  (None, 187, 128)          66560     \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 187, 128)          131584    \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 128)               131584    \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 128)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 512)               66048     \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 512)               262656    \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 1)                 513       \n=================================================================\nTotal params: 658,945\nTrainable params: 658,945\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"history = model_lstm_ptb.fit(x_train_ptb, out_train_ptb, epochs=40, batch_size=32, \n                             class_weight=class_weight, validation_data=(x_valid_ptb, out_valid_ptb),  \n                             callbacks=[checkpoint_cb, earlystop_cb])","metadata":{"execution":{"iopub.status.busy":"2021-07-03T22:10:12.569178Z","iopub.execute_input":"2021-07-03T22:10:12.569594Z","iopub.status.idle":"2021-07-03T23:02:08.600303Z","shell.execute_reply.started":"2021-07-03T22:10:12.569552Z","shell.execute_reply":"2021-07-03T23:02:08.598856Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Epoch 1/40\n310/310 [==============================] - 266s 834ms/step - loss: 0.7003 - f1_score: 0.8323 - val_loss: 0.6881 - val_f1_score: 0.8382\nEpoch 2/40\n310/310 [==============================] - 258s 834ms/step - loss: 0.6964 - f1_score: 0.8360 - val_loss: 0.6859 - val_f1_score: 0.8382\nEpoch 3/40\n310/310 [==============================] - 259s 835ms/step - loss: 0.6916 - f1_score: 0.8403 - val_loss: 0.7049 - val_f1_score: 0.8382\nEpoch 4/40\n310/310 [==============================] - 259s 834ms/step - loss: 0.6975 - f1_score: 0.8348 - val_loss: 0.6946 - val_f1_score: 0.8382\nEpoch 5/40\n310/310 [==============================] - 258s 834ms/step - loss: 0.6940 - f1_score: 0.8381 - val_loss: 0.6850 - val_f1_score: 0.8382\nEpoch 6/40\n310/310 [==============================] - 258s 834ms/step - loss: 0.6950 - f1_score: 0.8374 - val_loss: 0.6872 - val_f1_score: 0.8382\nEpoch 7/40\n310/310 [==============================] - 261s 843ms/step - loss: 0.6978 - f1_score: 0.8345 - val_loss: 0.6723 - val_f1_score: 0.8382\nEpoch 8/40\n310/310 [==============================] - 260s 839ms/step - loss: 0.6923 - f1_score: 0.8399 - val_loss: 0.6950 - val_f1_score: 0.8382\nEpoch 9/40\n310/310 [==============================] - 260s 838ms/step - loss: 0.6839 - f1_score: 0.8466 - val_loss: 0.7002 - val_f1_score: 0.8382\nEpoch 10/40\n310/310 [==============================] - 259s 835ms/step - loss: 0.6957 - f1_score: 0.8364 - val_loss: 0.6904 - val_f1_score: 0.8382\nEpoch 11/40\n310/310 [==============================] - 258s 833ms/step - loss: 0.6980 - f1_score: 0.8344 - val_loss: 0.6924 - val_f1_score: 0.8382\nEpoch 12/40\n310/310 [==============================] - 259s 836ms/step - loss: 0.6995 - f1_score: 0.8329 - val_loss: 0.6905 - val_f1_score: 0.8382\n","output_type":"stream"}]},{"cell_type":"code","source":"model_lstm_ptb.evaluate(x_test_ptb,out_test_ptb)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T23:02:08.602915Z","iopub.execute_input":"2021-07-03T23:02:08.603237Z","iopub.status.idle":"2021-07-03T23:02:16.094432Z","shell.execute_reply.started":"2021-07-03T23:02:08.603209Z","shell.execute_reply":"2021-07-03T23:02:16.093511Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"69/69 [==============================] - 7s 108ms/step - loss: 0.6720 - f1_score: 0.8398\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"[0.6720266938209534, 0.8397554755210876]"},"metadata":{}}]},{"cell_type":"code","source":"LSTM_pred_ptb = model_lstm_ptb.predict (x_test_ptb)\nLSTM_pred_ptb = np.rint(LSTM_pred_ptb.reshape(len(LSTM_pred_ptb)))\n\nprint(classification_report(out_test_ptb, LSTM_pred_ptb, target_names=[PTB_Outcome[i] for i in PTB_Outcome]))","metadata":{"execution":{"iopub.status.busy":"2021-07-03T23:02:16.095897Z","iopub.execute_input":"2021-07-03T23:02:16.096503Z","iopub.status.idle":"2021-07-03T23:02:24.131484Z","shell.execute_reply.started":"2021-07-03T23:02:16.096457Z","shell.execute_reply":"2021-07-03T23:02:24.130347Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n      Normal       0.00      0.00      0.00       603\n    Abnormal       0.72      1.00      0.84      1580\n\n    accuracy                           0.72      2183\n   macro avg       0.36      0.50      0.42      2183\nweighted avg       0.52      0.72      0.61      2183\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]}]}