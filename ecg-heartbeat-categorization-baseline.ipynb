{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading the data and exploring its shape and values","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-29T00:54:18.085489Z","iopub.execute_input":"2021-06-29T00:54:18.085821Z","iopub.status.idle":"2021-06-29T00:54:18.09624Z","shell.execute_reply.started":"2021-06-29T00:54:18.085788Z","shell.execute_reply":"2021-06-29T00:54:18.09517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mit_test = pd.read_csv('/kaggle/input/heartbeat/mitbih_test.csv',header=None)\nmit_train = pd.read_csv('/kaggle/input/heartbeat/mitbih_train.csv', header=None)\nptb_abnormal = pd.read_csv('/kaggle/input/heartbeat/ptbdb_abnormal.csv', header=None)\nptb_normal = pd.read_csv('/kaggle/input/heartbeat/ptbdb_normal.csv', header=None)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:18.097935Z","iopub.execute_input":"2021-06-29T00:54:18.09854Z","iopub.status.idle":"2021-06-29T00:54:24.066912Z","shell.execute_reply.started":"2021-06-29T00:54:18.0985Z","shell.execute_reply":"2021-06-29T00:54:24.066068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mit_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:24.068543Z","iopub.execute_input":"2021-06-29T00:54:24.06888Z","iopub.status.idle":"2021-06-29T00:54:24.097173Z","shell.execute_reply.started":"2021-06-29T00:54:24.068829Z","shell.execute_reply":"2021-06-29T00:54:24.096395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mit_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:24.098596Z","iopub.execute_input":"2021-06-29T00:54:24.098929Z","iopub.status.idle":"2021-06-29T00:54:24.124613Z","shell.execute_reply.started":"2021-06-29T00:54:24.098896Z","shell.execute_reply":"2021-06-29T00:54:24.123694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ptb_abnormal.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:24.126035Z","iopub.execute_input":"2021-06-29T00:54:24.126405Z","iopub.status.idle":"2021-06-29T00:54:24.150916Z","shell.execute_reply.started":"2021-06-29T00:54:24.126368Z","shell.execute_reply":"2021-06-29T00:54:24.150205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ptb_normal.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:24.153278Z","iopub.execute_input":"2021-06-29T00:54:24.153515Z","iopub.status.idle":"2021-06-29T00:54:24.1776Z","shell.execute_reply.started":"2021-06-29T00:54:24.153491Z","shell.execute_reply":"2021-06-29T00:54:24.176911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mit_test.rename(columns={187:\"Class\"}, inplace=True)\nmit_train.rename(columns={187:\"Class\"}, inplace=True)\nptb_abnormal.rename(columns={187:\"Class\"}, inplace=True)\nptb_normal.rename(columns={187:\"Class\"}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:24.180476Z","iopub.execute_input":"2021-06-29T00:54:24.180713Z","iopub.status.idle":"2021-06-29T00:54:24.18786Z","shell.execute_reply.started":"2021-06-29T00:54:24.18069Z","shell.execute_reply":"2021-06-29T00:54:24.186803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at how many classes are there in each dataset\nThe MIT dataset has 5 clases:\n* 0 = N  (Normal Beat)\n* 1 = S  (Supraventricular premature beat)\n* 2 = V  (Premature ventricular contraction)\n* 3 = F  (Fusion of ventricular and normal beat)\n* 4 = Q  (Unclassifiable beat)\n\nCompared to the PTB dataset which is 1 for abnormal and 0 for normal\n","metadata":{}},{"cell_type":"code","source":"print (\"MIT Train classes: \\n\", mit_train[\"Class\"].value_counts())\nprint (\"\\nMIT Test classes: \\n\", mit_test[\"Class\"].value_counts())\nprint (\"\\nPTB Abnormal classes: \\n\", ptb_abnormal[\"Class\"].value_counts())\nprint (\"\\nPTB Normal classes: \\n\", ptb_normal[\"Class\"].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:24.190263Z","iopub.execute_input":"2021-06-29T00:54:24.190936Z","iopub.status.idle":"2021-06-29T00:54:24.208204Z","shell.execute_reply.started":"2021-06-29T00:54:24.190894Z","shell.execute_reply":"2021-06-29T00:54:24.207373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting Dictionary to define the type of Heartbeat for both datasets\nMIT_Outcome = {0. : 'Normal Beat',\n               1. : 'Supraventricular premature beat',\n               2. : 'Premature ventricular contraction',\n               3. : 'Fusion of ventricular and normal beat',\n               4. : 'Unclassifiable beat'}\nPTB_Outcome = {0. : 'Normal',\n               1. : 'Abnormal'}","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:24.20944Z","iopub.execute_input":"2021-06-29T00:54:24.209837Z","iopub.status.idle":"2021-06-29T00:54:24.215828Z","shell.execute_reply.started":"2021-06-29T00:54:24.2098Z","shell.execute_reply":"2021-06-29T00:54:24.215123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating Plots of some of the samples in the dataset","metadata":{}},{"cell_type":"code","source":"#Plotting 10 random samples from the MIT training dataset with their classification\nplt.figure(figsize=(25,10))\nnp_count = np.linspace(0,186,187)\nnp_time = np.tile(np_count,(10,1))\nrnd = np.random.randint(0,mit_train.shape[0],size=(10,))\n\n\nfor i in range(np_time.shape[0]):\n    ax = plt.subplot(2,5,i+1)\n    ax.plot(mit_train.iloc[rnd[i],np_time[i,:]])\n    ax.set_title(MIT_Outcome[mit_train.loc[rnd[i],'Class']])\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:24.216941Z","iopub.execute_input":"2021-06-29T00:54:24.217284Z","iopub.status.idle":"2021-06-29T00:54:25.378024Z","shell.execute_reply.started":"2021-06-29T00:54:24.217251Z","shell.execute_reply":"2021-06-29T00:54:25.377079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting 10 random samples from the PTB training dataset with their classification\nplt.figure(figsize=(25,10))\nrnd = np.random.randint(0,ptb_normal.shape[0],size=(5,))\nrnd1 = np.random.randint(0,ptb_abnormal.shape[0], size=(5,))\n\n\nfor i in range(np_time.shape[0]):\n    ax = plt.subplot(2,5,i+1)\n    if (i < 5):\n        ax.plot(ptb_normal.iloc[rnd[i],np_time[i,:]])\n        ax.set_title(PTB_Outcome[ptb_normal.loc[rnd[i],'Class']])\n    else:\n        ax.plot(ptb_abnormal.iloc[rnd1[i-5],np_time[i,:]])\n        ax.set_title(PTB_Outcome[ptb_abnormal.loc[rnd1[i-5],'Class']])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:25.379409Z","iopub.execute_input":"2021-06-29T00:54:25.37989Z","iopub.status.idle":"2021-06-29T00:54:26.463964Z","shell.execute_reply.started":"2021-06-29T00:54:25.37985Z","shell.execute_reply":"2021-06-29T00:54:26.462951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experimenting with Classifiers for PTB Dataset","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import normalize\nfrom sklearn.svm import SVC \nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\n\nptb_full = pd.concat([ptb_normal, ptb_abnormal], axis=0).sample(frac=1)\nptb_full = ptb_full.apply(np.random.permutation)\nlearn_ptb, test_ptb, out_learn_ptb, out_test_ptb = train_test_split(ptb_full.iloc[:,:187], ptb_full.iloc[:,-1], test_size=0.15, random_state=42)\ntrain_ptb, valid_ptb, out_train_ptb, out_valid_ptb = train_test_split(learn_ptb, out_learn_ptb, test_size=0.2, random_state=42 )","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:26.465369Z","iopub.execute_input":"2021-06-29T00:54:26.46572Z","iopub.status.idle":"2021-06-29T00:54:26.581661Z","shell.execute_reply.started":"2021-06-29T00:54:26.46568Z","shell.execute_reply":"2021-06-29T00:54:26.580819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Traing dataset size: \", train_ptb.shape)\nprint(\"Validation dataset size: \", valid_ptb.shape)\nprint(\"Test dataset size: \", test_ptb.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:26.582887Z","iopub.execute_input":"2021-06-29T00:54:26.58326Z","iopub.status.idle":"2021-06-29T00:54:26.589514Z","shell.execute_reply.started":"2021-06-29T00:54:26.583225Z","shell.execute_reply":"2021-06-29T00:54:26.588429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Normalizing the training & test data \ntrain_ptb = normalize(train_ptb, axis=0, norm='max')\nvalid_ptb = normalize(valid_ptb, axis=0, norm='max')\ntest_ptb = normalize(test_ptb, axis=0, norm='max')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:26.59071Z","iopub.execute_input":"2021-06-29T00:54:26.591084Z","iopub.status.idle":"2021-06-29T00:54:26.621272Z","shell.execute_reply.started":"2021-06-29T00:54:26.591046Z","shell.execute_reply":"2021-06-29T00:54:26.620459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validating that the training data has a sample from both classess\nnp.unique(out_train_ptb)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:26.622568Z","iopub.execute_input":"2021-06-29T00:54:26.622915Z","iopub.status.idle":"2021-06-29T00:54:26.629282Z","shell.execute_reply.started":"2021-06-29T00:54:26.622879Z","shell.execute_reply":"2021-06-29T00:54:26.628297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Running SVM\n\nInitially we reduce the dataset by using PCA (Principal Component Analysis), the intution here is that there are 187 data points in each record but a lot of them flatten out much earlier than the sampled time period is complete.  This will be used to process other models going forward\n\nWe then create a SVM pipeline with and search  for the best parameters to feed the model.  ","metadata":{}},{"cell_type":"code","source":"#Looking at the plots we can see that there are a lot \"zero\" values which will not likely help our classification.  Eyeballing the data I chose 100 features to keep.\n#pca = PCA(n_components=100, random_state=42, whiten=True)\nsvc = SVC(kernel='rbf', class_weight='balanced')\n#clf_svc = make_pipeline(pca, svc)\n\nparam_grid = {'C': [1, 5, 10]}\ngrid_svc = GridSearchCV (svc, param_grid, verbose=2, scoring='f1_micro')\n# Train the grid of models. Time this process.\n%time grid_svc.fit(train_ptb, out_train_ptb)\n# Print the parameters which yield the best model performance\nprint (grid_svc.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:26.630768Z","iopub.execute_input":"2021-06-29T00:54:26.6312Z","iopub.status.idle":"2021-06-29T00:54:34.696517Z","shell.execute_reply.started":"2021-06-29T00:54:26.631164Z","shell.execute_reply":"2021-06-29T00:54:34.689325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting the best parameters from the previos GridSearchCV and predicting values on our validation set.\nsvc = grid_svc.best_estimator_\npred_svc = svc.predict(valid_ptb)\n\n\nprint(classification_report(out_valid_ptb, pred_svc, target_names=[PTB_Outcome[i] for i in PTB_Outcome]))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:34.699628Z","iopub.status.idle":"2021-06-29T00:54:34.702203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#validating that the predictions contained both classes\nnp.unique(pred_svc)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:34.706829Z","iopub.status.idle":"2021-06-29T00:54:34.709997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVM observations:\n\nIts interesting to note that the Support Vector Classifier did much better at predicting the abnormal class compared to the normal class.  Its worth noting that our original dataset contained ~72% Abnormal cases so it was probably able to learn better from those examples.","metadata":{}},{"cell_type":"markdown","source":"# Running ExtraTreesClassifier\n\nConsidering that this is a time series dataset and how the prior value impacts the current value our intuition is that this model would perform badly as \"randomly\" selecting features and making decisions based on these values would make for an archiac model.","metadata":{}},{"cell_type":"code","source":"forest = ExtraTreesClassifier (criterion='entropy', max_samples=10, class_weight='balanced', random_state=42)\n\nparam_grid = {'n_estimators': [10, 20, 30],\n             'max_depth' : [5, 10, 15, 20]}\ngrid_forest = GridSearchCV(forest, param_grid, scoring='f1_micro', verbose=2)\n\ngrid_forest.fit(train_ptb, out_train_ptb)\n\nprint(grid_forest.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:34.714684Z","iopub.status.idle":"2021-06-29T00:54:34.716938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the best estimator from the GridSearchCV into our model\nforest = grid_forest.best_estimator_\n\n# predicting the outcome by using the best model\npred_forest = forest.predict(valid_ptb)\nprint(classification_report(out_valid_ptb, pred_forest, target_names=[PTB_Outcome[i] for i in PTB_Outcome]))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:34.719122Z","iopub.status.idle":"2021-06-29T00:54:34.724064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validating that the model wasn't able to predict any record as normal\nnp.unique(pred_forest)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:34.725273Z","iopub.status.idle":"2021-06-29T00:54:34.726034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ExtraTreesClassifier Observation\n\nOut intution was correct and the model performed poorly.  It never predicted a normal case which makes this model useless","metadata":{}},{"cell_type":"markdown","source":"# Running Logistic Regression\n\nNow we run the standard Logistic Regression model, our intuition is that it would perform well since it will take the data as is (without randomization) and just try to predict an outcome.  The results should be comparable to SVM","metadata":{}},{"cell_type":"code","source":"logistic = LogisticRegression(random_state=42, class_weight='balanced')\n#clf_log = make_pipeline(pca, logistic)\n\nlogistic.fit(train_ptb, out_train_ptb)\npred_log = logistic.predict(valid_ptb)\nprint(classification_report(out_valid_ptb, pred_log, target_names=[PTB_Outcome[i] for i in PTB_Outcome]))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:34.727156Z","iopub.status.idle":"2021-06-29T00:54:34.727896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression observation:\n\nThe model didn't perform quite as well as SVM but much better than the Extra Tress Classifier.  \n\n","metadata":{}},{"cell_type":"markdown","source":"# Running TPOTClassifier to determine best algorithm\n### This part of the code takes roughly 3.5-4 hours to run, it has been commented out to avoid the long run times.  \n#The results are below, you can convert this cell from markdown to code to run it if desired\n\n#===================    Begin Code here ======================================\nfrom tpot import TPOTClassifier\n\ntpot = TPOTClassifier (generations=5, population_size=40, verbosity=2, random_state=42, scoring='f1_micro')\ntpot.fit(train_ptb, out_train_ptb)\n\n#evaluate the classifier against the validation set\nprint(tpot.score(valid_ptb, out_valid_ptb))\n\n#export the model to a file\ntpot.export('PTB_Data_Classifier.py')\n\n#===================   End Code here ==========================================","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:28:21.30313Z","iopub.execute_input":"2021-06-26T14:28:21.303742Z","iopub.status.idle":"2021-06-26T18:52:12.288323Z","shell.execute_reply.started":"2021-06-26T14:28:21.303704Z","shell.execute_reply":"2021-06-26T18:52:12.287387Z"}}},{"cell_type":"markdown","source":"======================== OUTPUT =============================\n\n\nGeneration 1 - Current best internal CV score: 0.7220818595250127\n\nGeneration 2 - Current best internal CV score: 0.7220818595250127\n\nGeneration 3 - Current best internal CV score: 0.7220818595250127\n\nGeneration 4 - Current best internal CV score: 0.7220818595250127\n\nGeneration 5 - Current best internal CV score: 0.7220818595250127\n\nBest pipeline: RandomForestClassifier(input_matrix, bootstrap=False, criterion=entropy, max_features=0.1, min_samples_leaf=5, min_samples_split=15, n_estimators=100)\n0.715844785772029\n\n\n\n\n\n================== END OF OUTPUT =============================","metadata":{}},{"cell_type":"markdown","source":"########  PTB_Data_Classifier.Py File content ########\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n#NOTE: Make sure that the outcome column is labeled 'target' in the data file\n\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\nfeatures = tpot_data.drop('target', axis=1)\ntraining_features, testing_features, training_target, testing_target = \\\n            train_test_split(features, tpot_data['target'], random_state=42)\n\n#Average CV score on the training set was: 0.7220818595250127\n\nexported_pipeline = RandomForestClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.1, min_samples_leaf=5, min_samples_split=15, n_estimators=100)\n#Fix random state in exported estimator\nif hasattr(exported_pipeline, 'random_state'):\n    setattr(exported_pipeline, 'random_state', 42)\n\nexported_pipeline.fit(training_features, training_target)\nresults = exported_pipeline.predict(testing_features)\n\n\n########  End of File content ########","metadata":{}},{"cell_type":"markdown","source":"# Running RandomForestClassifer\n\nFinally lets run the Running RandomForestClassifer that AutoML has recommended for us and see what we observer.  We're expecting this to be our best performing model so far.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrndforest = RandomForestClassifier(bootstrap=False, criterion='entropy', min_samples_leaf=2, min_samples_split=2, n_estimators=100)\nrndforest.fit(train_ptb, out_train_ptb)\npred_rndforest = rndforest.predict(valid_ptb)\n\nprint(classification_report(out_valid_ptb, pred_rndforest, target_names=[PTB_Outcome[i] for i in PTB_Outcome]))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:34.735761Z","iopub.status.idle":"2021-06-29T00:54:34.736508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analayzing the MIT Heartbeat Data\nNow we repeat the same analysis with the different dataset and tweak accordingly","metadata":{}},{"cell_type":"code","source":"# Since the MIT dataset already comes as a train set and test set, we just split 20% of the training set for validation\ntrain_mit, valid_mit, out_train_mit, out_valid_mit = train_test_split(mit_train.iloc[:,:187], mit_train.iloc[:,-1], test_size=0.20, random_state=42)\n\n#we remove the targets from the test set\ntest_mit, out_test_mit = mit_test.iloc[:,:187], mit_test.iloc[:,-1]\n\n#Normalizing the training & test data \ntrain_mit = normalize(train_mit, axis=0, norm='max')\nvalid_mit = normalize(valid_mit, axis=0, norm='max')\ntest_mit = normalize(test_ptb, axis=0, norm='max')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:34.737632Z","iopub.status.idle":"2021-06-29T00:54:34.738376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looking at the plots we can see that there are a lot \"zero\" values which will not likely help our classification.  Eyeballing the data I chose 100 features to keep.\n#pca = PCA(n_components=100, random_state=42, whiten=True)\nsvc = SVC(kernel='rbf', class_weight='balanced')\n#clf_svc = make_pipeline(pca, svc)\n\nparam_grid = {'C': [1, 5, 10]}\ngrid_svc = GridSearchCV (svc, param_grid, verbose=2, scoring='f1_micro')\n# Train the grid of models. Time this process.\n%time grid_svc.fit(train_mit, out_train_mit)\n# Print the parameters which yield the best model performance\nprint (grid_svc.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:34.739494Z","iopub.status.idle":"2021-06-29T00:54:34.740234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting the best parameters from the previos GridSearchCV and predicting values on our validation set.\nsvc = grid_svc.best_estimator_\npred_svc_mit = svc.predict(valid_mit)\n\n\nprint(classification_report(out_valid_mit, pred_svc_mit, target_names=[MIT_Outcome[i] for i in MIT_Outcome]))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:34.741378Z","iopub.status.idle":"2021-06-29T00:54:34.742135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forest_mit = ExtraTreesClassifier (criterion='entropy', max_samples=10, class_weight='balanced', random_state=42)\n\nparam_grid = {'n_estimators': [10, 20, 30],\n             'max_depth' : [5, 10, 15, 20]}\ngrid_forest_mit = GridSearchCV(forest_mit, param_grid, scoring='f1_micro', verbose=2)\n\ngrid_forest_mit.fit(train_mit, out_train_mit)\n\nprint(grid_forest_mit.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:34.743266Z","iopub.status.idle":"2021-06-29T00:54:34.75041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the best estimator from the GridSearchCV into our model\nforest_mit = grid_forest_mit.best_estimator_\n\n# predicting the outcome by using the best model\npred_forest_mit = forest_mit.predict(valid_mit)\nprint(classification_report(out_valid_mit, pred_forest_mit, target_names=[MIT_Outcome[i] for i in MIT_Outcome]))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:34.751737Z","iopub.status.idle":"2021-06-29T00:54:34.752549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logistic_mit = LogisticRegression(random_state=42, class_weight='balanced', max_iter=10000)\n#clf_log = make_pipeline(pca, logistic)\n\nlogistic_mit.fit(train_mit, out_train_mit)\npred_log_mit = logistic_mit.predict(valid_mit)\nprint(classification_report(out_valid_mit, pred_log_mit, target_names=[MIT_Outcome[i] for i in MIT_Outcome]))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:54:34.753746Z","iopub.status.idle":"2021-06-29T00:54:34.754639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}